{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols, mixedlm\n",
    "import statsmodels.formula.api as smf\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/scores.parquet').reset_index(drop=True).replace({\n",
    "    \"trec23\": \"2014\",\n",
    "    \"trec22\": \"2013\",\n",
    "    \"trec21\": \"2012\",\n",
    "    \"trec20\": \"2011\",\n",
    "    \"trec19\": \"2010\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NDCG Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.merge(\n",
    "        (\n",
    "            df\n",
    "            .loc[:,[\"WDCG\",\"TREC\",\"Topic\"]]\n",
    "            .groupby([\"TREC\",\"Topic\"])\n",
    "            .min()\n",
    "            .groupby([\"TREC\"])\n",
    "            .apply(lambda group: len(list(filter(None, map(lambda value: value < 0, group['WDCG'])))) / len(group))\n",
    "            .reset_index()\n",
    "            .rename({0: \"≤ 0\"}, axis=1)\n",
    "        ),\n",
    "        (\n",
    "            df\n",
    "            .loc[:,[\"WDCG\",\"TREC\",\"Topic\"]]\n",
    "            .groupby([\"TREC\",\"Topic\"])\n",
    "            .min()\n",
    "            .groupby([\"TREC\"])\n",
    "            .apply(lambda group: len(list(filter(None, map(lambda value: value < -1, group['WDCG'])))) / len(group))\n",
    "            .reset_index()\n",
    "            .rename({0: \"≤ -1\"}, axis=1)\n",
    "        )\n",
    "    )\n",
    "    .set_index(\"TREC\")\n",
    "    .round(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .groupby([\"TREC\",\"K\"])\n",
    "    .corr(method='spearman')\n",
    "    .NDCG_min\n",
    "    .unstack()\n",
    "    .NDCG_org\n",
    "    .reset_index()\n",
    "    .pivot(\"TREC\",\"K\",\"NDCG_org\")\n",
    "    .round(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability Evaluation\n",
    "Adapted from *Evangelos Kanoulas, Javed A. Aslam: Empirical justification of the gain and discount function for nDCG. CIKM 2009: 611-620*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Brennan (2001): Generalizability Theory, New York: Springer. Eq. 3.1\n",
    "# \"Score ~ (1 | Person) + (1 | Task) + (1 | Person:Task)\" \n",
    "# Task -> Topic, Person -> Run\n",
    "f = 'Score ~ Run + Topic + Run*Topic -1'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Method Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependability(data):\n",
    "    a = data.Topic.nunique()\n",
    "    b = data.Run.nunique()\n",
    "    data[\"Group\"] = 1  \n",
    "\n",
    "    result = ols(formula = 'Score ~ Run + Topic + Run*Topic -1', data=data).fit()\n",
    "\n",
    "    table = sm.stats.anova_lm(result, typ=1)\n",
    "    var_sys = abs(table.loc['Run','mean_sq'] - table.loc['Run:Topic','mean_sq'])/a\n",
    "    var_topic = abs(table.loc['Topic','mean_sq'] - table.loc['Run:Topic','mean_sq'])/b\n",
    "    var_sys_topic = abs(table.loc['Run:Topic','mean_sq'] - table.loc['Residual','mean_sq'])\n",
    "    \n",
    "    return var_sys / (var_sys + var_topic + var_sys_topic/a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        (\n",
    "            df\n",
    "            .rename({'NDCG_org':'Score'},axis=1)\n",
    "            .groupby(['TREC','K'])\n",
    "            .apply(dependability)\n",
    "            .reset_index()\n",
    "            .rename({0:'NDCG_org'},axis=1)\n",
    "            .pivot(\"TREC\",\"K\",\"NDCG_org\")\n",
    "        ),\n",
    "        (\n",
    "            df\n",
    "            .rename({'NDCG_min':'Score'},axis=1)\n",
    "            .groupby(['TREC','K'])\n",
    "            .apply(dependability)\n",
    "            .reset_index()\n",
    "            .rename({0:'NDCG_min'},axis=1)\n",
    "            .pivot(\"TREC\",\"K\",\"NDCG_min\")\n",
    "        ),\n",
    "        (\n",
    "            df\n",
    "            .rename({'NDCG_0':'Score'},axis=1)\n",
    "            .groupby(['TREC','K'])\n",
    "            .apply(dependability)\n",
    "            .reset_index()\n",
    "            .rename({0:'NDCG_0'},axis=1)\n",
    "            .pivot(\"TREC\",\"K\",\"NDCG_0\")\n",
    "        )\n",
    "    ],\n",
    "    keys = [\"NDCG_org\", \"NDCG_min\", \"NDCG_0\"]\n",
    ").round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Evaluation \n",
    "*Tetsuya Sakai: Evaluating evaluation metrics based on the bootstrap. SIGIR 2006: 525-532*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Method Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(data):\n",
    "    sn = np.sqrt(len(data))\n",
    "    z = data.X - data.Y\n",
    "    w = z - z.mean()\n",
    "    t_z = abs(z.mean() / (z.std() / sn))\n",
    "\n",
    "    res = (\n",
    "        sum(\n",
    "            map(\n",
    "                lambda w: 1 if abs(w.mean() / (w.std() / sn)) >= t_z  else 0,\n",
    "                [w.sample(frac = 1, replace = True) for b in range(1,B+1)]\n",
    "            )\n",
    "        )\n",
    "        /B\n",
    "    )\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Applied to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_data = []\n",
    "for metric in [\"NDCG_org\",\"NDCG_0\",\"NDCG_min\"]:\n",
    "    for trec in df.TREC.unique():\n",
    "        for k in df.K.unique():\n",
    "            for x,y in itertools.combinations(df[df.TREC == trec].Run.unique(),2):\n",
    "                data = (\n",
    "                    pd.merge(\n",
    "                        df.loc[(df.TREC == trec) & (df.K == k) & (df.Run == x), [metric,'Topic']].drop_duplicates(),\n",
    "                        df.loc[(df.TREC == trec) & (df.K == k) & (df.Run == y), [metric,'Topic']].drop_duplicates(),\n",
    "                        on = 'Topic'\n",
    "                    )\n",
    "                    .loc[:,[metric+'_x',metric+'_y']]\n",
    "                    .rename({metric+'_x':'X', metric+'_y':'Y'}, axis = 1)\n",
    "                )\n",
    "                bootstrap_data.append({'Metric': metric, 'TREC': trec, 'K': k, 'X': x,'Y': y, 'ASL': bootstrap(data)})\n",
    "                \n",
    "bootstrap_data = pd.DataFrame(bootstrap_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(bootstrap_data, col=\"K\", row=\"TREC\",  hue = \"Metric\", legend_out=True, height=1.75, aspect=1.5)\n",
    "g.map(\n",
    "    sns.distplot, \n",
    "    \"ASL\", \n",
    "    bins=np.arange(0,0.25,0.001), \n",
    "    kde=False, \n",
    "    hist_kws={\n",
    "        \"cumulative\":True, \n",
    "        \"histtype\": \"step\", \n",
    "        \"alpha\": 1, \n",
    "    }, \n",
    "    rug=False,\n",
    "    norm_hist=True,\n",
    ").add_legend()\n",
    "g.set_axis_labels(\"Level\", \"Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stability Evaluation\n",
    "*Chris Buckley, Ellen M. Voorhees: Evaluating Evaluation Measure Stability. SIGIR Forum 51(2): 235-242 (2017)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 0.05\n",
    "m = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Method Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(data):\n",
    "    result = []\n",
    "    for i in range(0,m):\n",
    "        for n in range(5, data.Topic.nunique() + 1):\n",
    "            print(i,n,end=\" \\r\")\n",
    "            topics = data.Topic.sample(n = n)\n",
    "            for x, y in itertools.combinations(data.Run.unique(), 2):\n",
    "                X = data[(data.Topic.isin(topics)) & (data.Run == x)].set_index('Topic').Score\n",
    "                Y = data[(data.Topic.isin(topics)) & (data.Run == y)].set_index('Topic').Score\n",
    "                x_better = sum(X > (Y + Y*f))\n",
    "                y_better = sum(Y > (X + X*f))\n",
    "                result.append({'X': x,'Y': y, 'N': n, 'M': i, 'X Better': x_better, 'Y Better': y_better})\n",
    "    return (\n",
    "        pd.DataFrame(result)\n",
    "        .groupby(['N','M'])\n",
    "        .apply(lambda group: pd.Series([min(row[1]['X Better'],row[1]['Y Better'])/row[1]['N'] for row in group.iterrows()]).mean())\n",
    "        .groupby('N')\n",
    "        .mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Applied to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (\n",
    "    df\n",
    "    .loc[df.Run != 'uogTrB47Vm',['TREC','K','Topic','Run','NDCG_org','NDCG_0', 'NDCG_min']]\n",
    "    .melt(id_vars = ['TREC','Topic','K','Run'], var_name = 'Metric', value_name = 'Score')\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "data_pool = []\n",
    "\n",
    "i = 0\n",
    "for metric in tmp.Metric.unique():\n",
    "    for trec in tmp.TREC.unique():\n",
    "        for k in tmp.K.unique():\n",
    "            data_pool.append((i,tmp.loc[(tmp.K == k) & (tmp.TREC == trec) & (tmp.Metric == metric),:].reset_index()))\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "def execution_wrapper(data):\n",
    "    res = error_rate(data[1])\n",
    "    res = pd.DataFrame(res)\n",
    "    res['TREC'] = data[1].TREC.unique()[0]\n",
    "    res['K'] = data[1].K.unique()[0]\n",
    "    res['Metric'] = data[1].Metric.unique()[0]\n",
    "    return res\n",
    "\n",
    "threads = ThreadPool(8) \n",
    "results = threads.map(execution_wrapper, data_pool)\n",
    "tmp = (\n",
    "    pd.concat(results)\n",
    "    .reset_index()\n",
    "    .rename({0:'Error Rate'}, axis = 1)\n",
    "    .loc[:,['TREC','K','Metric','N','Error Rate']]\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kw = {\"color\": [\"k\", \"k\", \"k\"], \"linestyle\" : [\"-\",\"--\",\":\"]}\n",
    "g = sns.FacetGrid(tmp.rename({\"N\":\"Number of Topics\"}, axis=1), row = \"TREC\", col = \"K\",  hue = \"Metric\", hue_kws=kw, legend_out=True, height=1.75, aspect=1.5)\n",
    "g = (g.map(plt.plot, \"Number of Topics\", \"Error Rate\").add_legend())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
